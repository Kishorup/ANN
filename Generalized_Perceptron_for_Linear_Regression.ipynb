{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPbMQWhmrEfylywkBW0cF7k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kishorup/ANN/blob/main/Generalized_Perceptron_for_Linear_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7EFpiLikyRcp",
        "outputId": "a53821d7-09ec-49db-e0dc-b441fdaef544"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Training Perceptron with n = 4 Features ---\n",
            "True weights: [-0.75592353 -0.00964618 -0.93122296  0.8186408 ], True bias: 5\n",
            "Epoch   1 | MSE: 45.322178\n",
            "Epoch   2 | MSE: 31.292244\n",
            "Epoch   3 | MSE: 21.687116\n",
            "Epoch   4 | MSE: 15.109641\n",
            "Epoch   5 | MSE: 10.603941\n",
            "Epoch   6 | MSE: 7.516029\n",
            "Epoch   7 | MSE: 5.398442\n",
            "Epoch   8 | MSE: 3.945015\n",
            "Epoch   9 | MSE: 2.946246\n",
            "Epoch  10 | MSE: 2.258774\n",
            "Epoch  11 | MSE: 1.784489\n",
            "Epoch  12 | MSE: 1.456240\n",
            "Epoch  13 | MSE: 1.228065\n",
            "Epoch  14 | MSE: 1.068500\n",
            "Epoch  15 | MSE: 0.956002\n",
            "Epoch  16 | MSE: 0.875815\n",
            "Epoch  17 | MSE: 0.817837\n",
            "Epoch  18 | MSE: 0.775142\n",
            "Epoch  19 | MSE: 0.742986\n",
            "Epoch  20 | MSE: 0.718114\n",
            "Epoch  21 | MSE: 0.698296\n",
            "Epoch  22 | MSE: 0.681997\n",
            "Epoch  23 | MSE: 0.668166\n",
            "Epoch  24 | MSE: 0.656080\n",
            "Epoch  25 | MSE: 0.645238\n",
            "Epoch  26 | MSE: 0.635299\n",
            "Epoch  27 | MSE: 0.626024\n",
            "Epoch  28 | MSE: 0.617250\n",
            "Epoch  29 | MSE: 0.608861\n",
            "Epoch  30 | MSE: 0.600778\n",
            "Epoch  31 | MSE: 0.592945\n",
            "Epoch  32 | MSE: 0.585322\n",
            "Epoch  33 | MSE: 0.577882\n",
            "Epoch  34 | MSE: 0.570604\n",
            "Epoch  35 | MSE: 0.563472\n",
            "Epoch  36 | MSE: 0.556477\n",
            "Epoch  37 | MSE: 0.549610\n",
            "Epoch  38 | MSE: 0.542864\n",
            "Epoch  39 | MSE: 0.536234\n",
            "Epoch  40 | MSE: 0.529716\n",
            "Epoch  41 | MSE: 0.523306\n",
            "Epoch  42 | MSE: 0.517001\n",
            "Epoch  43 | MSE: 0.510799\n",
            "Epoch  44 | MSE: 0.504698\n",
            "Epoch  45 | MSE: 0.498694\n",
            "Epoch  46 | MSE: 0.492786\n",
            "Epoch  47 | MSE: 0.486973\n",
            "Epoch  48 | MSE: 0.481251\n",
            "Epoch  49 | MSE: 0.475620\n",
            "Epoch  50 | MSE: 0.470078\n",
            "Epoch  51 | MSE: 0.464622\n",
            "Epoch  52 | MSE: 0.459253\n",
            "Epoch  53 | MSE: 0.453967\n",
            "Epoch  54 | MSE: 0.448763\n",
            "Epoch  55 | MSE: 0.443641\n",
            "Epoch  56 | MSE: 0.438599\n",
            "Epoch  57 | MSE: 0.433634\n",
            "Epoch  58 | MSE: 0.428746\n",
            "Epoch  59 | MSE: 0.423934\n",
            "Epoch  60 | MSE: 0.419196\n",
            "Epoch  61 | MSE: 0.414531\n",
            "Epoch  62 | MSE: 0.409938\n",
            "Epoch  63 | MSE: 0.405415\n",
            "Epoch  64 | MSE: 0.400961\n",
            "Epoch  65 | MSE: 0.396575\n",
            "Epoch  66 | MSE: 0.392256\n",
            "Epoch  67 | MSE: 0.388003\n",
            "Epoch  68 | MSE: 0.383815\n",
            "Epoch  69 | MSE: 0.379689\n",
            "Epoch  70 | MSE: 0.375627\n",
            "Epoch  71 | MSE: 0.371625\n",
            "Epoch  72 | MSE: 0.367684\n",
            "Epoch  73 | MSE: 0.363803\n",
            "Epoch  74 | MSE: 0.359979\n",
            "Epoch  75 | MSE: 0.356213\n",
            "Epoch  76 | MSE: 0.352503\n",
            "Epoch  77 | MSE: 0.348849\n",
            "Epoch  78 | MSE: 0.345248\n",
            "Epoch  79 | MSE: 0.341702\n",
            "Epoch  80 | MSE: 0.338208\n",
            "Epoch  81 | MSE: 0.334766\n",
            "Epoch  82 | MSE: 0.331374\n",
            "Epoch  83 | MSE: 0.328033\n",
            "Epoch  84 | MSE: 0.324741\n",
            "Epoch  85 | MSE: 0.321497\n",
            "Epoch  86 | MSE: 0.318301\n",
            "Epoch  87 | MSE: 0.315151\n",
            "Epoch  88 | MSE: 0.312048\n",
            "Epoch  89 | MSE: 0.308990\n",
            "Epoch  90 | MSE: 0.305976\n",
            "Epoch  91 | MSE: 0.303006\n",
            "Epoch  92 | MSE: 0.300079\n",
            "Epoch  93 | MSE: 0.297194\n",
            "Epoch  94 | MSE: 0.294351\n",
            "Epoch  95 | MSE: 0.291548\n",
            "Epoch  96 | MSE: 0.288786\n",
            "Epoch  97 | MSE: 0.286063\n",
            "Epoch  98 | MSE: 0.283380\n",
            "Epoch  99 | MSE: 0.280734\n",
            "Epoch 100 | MSE: 0.278127\n",
            "\n",
            "Learned Weights: [1.88664894 0.35646071 0.75226413 1.21679793]\n",
            "Learned Bias: 2.7567\n",
            "\n",
            "--- Training Perceptron with n = 5 Features ---\n",
            "True weights: [0.93916926 0.55026565 0.87899788 0.7896547  0.19579996], True bias: 5\n",
            "Epoch   1 | MSE: 67.777272\n",
            "Epoch   2 | MSE: 44.896643\n",
            "Epoch   3 | MSE: 29.754465\n",
            "Epoch   4 | MSE: 19.734363\n",
            "Epoch   5 | MSE: 13.104355\n",
            "Epoch   6 | MSE: 8.717947\n",
            "Epoch   7 | MSE: 5.816243\n",
            "Epoch   8 | MSE: 3.896938\n",
            "Epoch   9 | MSE: 2.627584\n",
            "Epoch  10 | MSE: 1.788166\n",
            "Epoch  11 | MSE: 1.233092\n",
            "Epoch  12 | MSE: 0.866034\n",
            "Epoch  13 | MSE: 0.623260\n",
            "Epoch  14 | MSE: 0.462618\n",
            "Epoch  15 | MSE: 0.356231\n",
            "Epoch  16 | MSE: 0.285668\n",
            "Epoch  17 | MSE: 0.238748\n",
            "Epoch  18 | MSE: 0.207421\n",
            "Epoch  19 | MSE: 0.186373\n",
            "Epoch  20 | MSE: 0.172095\n",
            "Epoch  21 | MSE: 0.162274\n",
            "Epoch  22 | MSE: 0.155383\n",
            "Epoch  23 | MSE: 0.150419\n",
            "Epoch  24 | MSE: 0.146721\n",
            "Epoch  25 | MSE: 0.143855\n",
            "Epoch  26 | MSE: 0.141538\n",
            "Epoch  27 | MSE: 0.139582\n",
            "Epoch  28 | MSE: 0.137866\n",
            "Epoch  29 | MSE: 0.136312\n",
            "Epoch  30 | MSE: 0.134867\n",
            "Epoch  31 | MSE: 0.133499\n",
            "Epoch  32 | MSE: 0.132186\n",
            "Epoch  33 | MSE: 0.130915\n",
            "Epoch  34 | MSE: 0.129677\n",
            "Epoch  35 | MSE: 0.128466\n",
            "Epoch  36 | MSE: 0.127280\n",
            "Epoch  37 | MSE: 0.126115\n",
            "Epoch  38 | MSE: 0.124970\n",
            "Epoch  39 | MSE: 0.123844\n",
            "Epoch  40 | MSE: 0.122737\n",
            "Epoch  41 | MSE: 0.121649\n",
            "Epoch  42 | MSE: 0.120578\n",
            "Epoch  43 | MSE: 0.119525\n",
            "Epoch  44 | MSE: 0.118489\n",
            "Epoch  45 | MSE: 0.117469\n",
            "Epoch  46 | MSE: 0.116467\n",
            "Epoch  47 | MSE: 0.115481\n",
            "Epoch  48 | MSE: 0.114511\n",
            "Epoch  49 | MSE: 0.113556\n",
            "Epoch  50 | MSE: 0.112617\n",
            "Epoch  51 | MSE: 0.111693\n",
            "Epoch  52 | MSE: 0.110784\n",
            "Epoch  53 | MSE: 0.109890\n",
            "Epoch  54 | MSE: 0.109009\n",
            "Epoch  55 | MSE: 0.108143\n",
            "Epoch  56 | MSE: 0.107290\n",
            "Epoch  57 | MSE: 0.106450\n",
            "Epoch  58 | MSE: 0.105624\n",
            "Epoch  59 | MSE: 0.104810\n",
            "Epoch  60 | MSE: 0.104009\n",
            "Epoch  61 | MSE: 0.103219\n",
            "Epoch  62 | MSE: 0.102442\n",
            "Epoch  63 | MSE: 0.101676\n",
            "Epoch  64 | MSE: 0.100922\n",
            "Epoch  65 | MSE: 0.100179\n",
            "Epoch  66 | MSE: 0.099447\n",
            "Epoch  67 | MSE: 0.098725\n",
            "Epoch  68 | MSE: 0.098014\n",
            "Epoch  69 | MSE: 0.097314\n",
            "Epoch  70 | MSE: 0.096623\n",
            "Epoch  71 | MSE: 0.095942\n",
            "Epoch  72 | MSE: 0.095271\n",
            "Epoch  73 | MSE: 0.094609\n",
            "Epoch  74 | MSE: 0.093956\n",
            "Epoch  75 | MSE: 0.093313\n",
            "Epoch  76 | MSE: 0.092678\n",
            "Epoch  77 | MSE: 0.092052\n",
            "Epoch  78 | MSE: 0.091434\n",
            "Epoch  79 | MSE: 0.090825\n",
            "Epoch  80 | MSE: 0.090224\n",
            "Epoch  81 | MSE: 0.089631\n",
            "Epoch  82 | MSE: 0.089046\n",
            "Epoch  83 | MSE: 0.088469\n",
            "Epoch  84 | MSE: 0.087899\n",
            "Epoch  85 | MSE: 0.087337\n",
            "Epoch  86 | MSE: 0.086781\n",
            "Epoch  87 | MSE: 0.086233\n",
            "Epoch  88 | MSE: 0.085692\n",
            "Epoch  89 | MSE: 0.085158\n",
            "Epoch  90 | MSE: 0.084631\n",
            "Epoch  91 | MSE: 0.084110\n",
            "Epoch  92 | MSE: 0.083596\n",
            "Epoch  93 | MSE: 0.083088\n",
            "Epoch  94 | MSE: 0.082587\n",
            "Epoch  95 | MSE: 0.082091\n",
            "Epoch  96 | MSE: 0.081602\n",
            "Epoch  97 | MSE: 0.081119\n",
            "Epoch  98 | MSE: 0.080641\n",
            "Epoch  99 | MSE: 0.080169\n",
            "Epoch 100 | MSE: 0.079703\n",
            "\n",
            "Learned Weights: [1.12870246 0.33041473 2.17918997 2.09252856 0.22630754]\n",
            "Learned Bias: 3.6361\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def generate_dataset(n_features, n_samples=10):\n",
        "    np.random.seed(42)\n",
        "    X = np.random.rand(n_samples, n_features)  # Inputs in [0, 1]\n",
        "    true_weights = np.random.uniform(-1, 1, size=n_features)  # True weights âˆˆ [-1, 1]\n",
        "    bias = 5\n",
        "    y = np.dot(X, true_weights) + bias\n",
        "    return X, y, true_weights, bias\n",
        "\n",
        "def train_perceptron(X, y, learning_rate=0.01, epochs=100):\n",
        "    n_samples, n_features = X.shape\n",
        "    weights = np.random.randn(n_features)\n",
        "    bias = np.random.randn()\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        total_error = 0\n",
        "        for i in range(n_samples):\n",
        "            x_i = X[i]\n",
        "            y_pred = np.dot(weights, x_i) + bias\n",
        "            error = y[i] - y_pred\n",
        "            total_error += error ** 2\n",
        "\n",
        "            # Update weights and bias\n",
        "            weights += learning_rate * error * x_i\n",
        "            bias += learning_rate * error\n",
        "\n",
        "        mse = total_error / n_samples\n",
        "        print(f\"Epoch {epoch:3d} | MSE: {mse:.6f}\")\n",
        "\n",
        "    return weights, bias\n",
        "\n",
        "def run_test(n_features):\n",
        "    print(f\"\\n--- Training Perceptron with n = {n_features} Features ---\")\n",
        "    X, y, true_w, true_b = generate_dataset(n_features)\n",
        "    print(f\"True weights: {true_w}, True bias: {true_b}\")\n",
        "    weights, bias = train_perceptron(X, y)\n",
        "    print(f\"\\nLearned Weights: {weights}\")\n",
        "    print(f\"Learned Bias: {bias:.4f}\")\n",
        "\n",
        "# Run for n = 4 and n = 5\n",
        "run_test(n_features=4)\n",
        "run_test(n_features=5)"
      ]
    }
  ]
}